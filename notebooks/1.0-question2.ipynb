{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1/ Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from typing import Dict, Callable\n",
    "\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/ Background Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Mutual Information Using Histograms\n",
    "\n",
    "### 1. **Understanding the Data and the Problem**:\n",
    "Let's assume you have data for two discrete random variables \\( X \\) and \\( Y \\). This data can be thought of as samples or observations of these variables.\n",
    "\n",
    "### 2. **Construct Histograms**:\n",
    "\n",
    "#### a. **Joint Histogram**:\n",
    "Construct a 2D histogram for \\( X \\) and \\( Y \\). In this 2D histogram:\n",
    "- The x-axis represents all possible values of \\( X \\).\n",
    "- The y-axis represents all possible values of \\( Y \\).\n",
    "- Each cell (or bin) in this 2D histogram represents a combination of values for \\( X \\) and \\( Y \\). The count in each bin represents how often that combination occurs in your data.\n",
    "\n",
    "#### b. **Marginal Histograms**:\n",
    "Construct individual histograms for \\( X \\) and \\( Y \\):\n",
    "- For \\( X \\): Count how often each value of \\( X \\) occurs in your data.\n",
    "- For \\( Y \\): Count how often each value of \\( Y \\) occurs in your data.\n",
    "\n",
    "### 3. **Normalize Histograms to Get Probabilities**:\n",
    "\n",
    "#### a. **Joint Probabilities**:\n",
    "To convert the joint histogram counts into probabilities, divide the count in each bin by the total number of data points. This will give you an estimate of the joint probability \\( P(X=x, Y=y) \\) for each combination of \\( x \\) and \\( y \\).\n",
    "\n",
    "#### b. **Marginal Probabilities**:\n",
    "Similarly, to get the marginal probabilities from the individual histograms:\n",
    "- For \\( X \\): Divide each bin's count by the total number of data points to get \\( P(X=x) \\).\n",
    "- For \\( Y \\): Do the same to get \\( P(Y=y) \\).\n",
    "\n",
    "### 4. **Compute Mutual Information**:\n",
    "\n",
    "Using the normalized histograms, you can calculate the MI with the following formula:\n",
    "\n",
    "\n",
    "$$ I(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} P_{X,Y}(x,y) \\log \\left( \\frac{P_{X,Y}(x,y)}{P_X(x) P_Y(y)} \\right) $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For each bin in the joint histogram:\n",
    "\n",
    "- Get the joint probability \\( P(X=x, Y=y) \\) from the normalized joint histogram.\n",
    "- Get the marginal probabilities \\( P(X=x) \\) and \\( P(Y=y) \\) from the normalized marginal histograms.\n",
    "- Use these values in the MI formula above to compute the contribution from this bin.\n",
    "- Sum up contributions from all bins to get the final MI value.\n",
    "\n",
    "### 5. **Interpret the Result**:\n",
    "\n",
    "The resulting MI value quantifies the amount of information shared between \\( X \\) and \\( Y \\):\n",
    "- An MI of 0 suggests \\( X \\) and \\( Y \\) are independent.\n",
    "- A higher MI indicates a higher dependency between \\( X \\) and \\( Y \\).\n",
    "\n",
    "### Points to Note:\n",
    "\n",
    "- The precision of your histograms (number and size of bins) can affect the MI calculation.\n",
    "- This method gives an estimate of MI based on empirical data. The true MI would require knowledge of the true underlying probability distributions.\n",
    "- Ensure you have enough data. Sparse data can lead to unreliable MI values.\n",
    "- This method is specifically for discrete variables. If you have continuous data, it needs to be discretized before this approach can be used. The way you discretize can also impact the result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3/ Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_h5_to_dataframe(h5_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read a .h5 file and convert all data into a pandas dataframe.\n",
    "    param h5_path: Path to location of .h5 file\n",
    "    return: a pandas dataframe\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    with h5py.File(h5_path, 'r') as file:\n",
    "        keys = list(file.keys())\n",
    "        for key in keys:\n",
    "            data[key] = file[key][:]\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def mutual_information_binary(X: pd.Series, Y: pd.Series) -> np.float64:\n",
    "    \"\"\"\n",
    "    A function to calculate the mutual information between two discrete variables,\n",
    "    assuming the two variables X and Y are binary or can be categorized into two groups.\n",
    "    This approach using normalized histograms to estimate the joint and marginal distributions.\n",
    "    param X: first variable\n",
    "    param Y: second variable\n",
    "    return: mutual information between X and Y\n",
    "    \"\"\"\n",
    "    joint_prob = np.histogram2d(X, Y, bins=2)[0] / len(X)\n",
    "    prob_X = np.histogram(X, bins=2)[0] / len(X)\n",
    "    prob_Y = np.histogram(Y, bins=2)[0] / len(Y)\n",
    "\n",
    "    mi = 0\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            if joint_prob[i][j] > 0:\n",
    "                mi += joint_prob[i][j] * np.log(joint_prob[i][j] / (prob_X[i] * prob_Y[j]))\n",
    "\n",
    "    return mi\n",
    "\n",
    "\n",
    "def mutual_information_multiple_discrete(X: pd.Series, Y: pd.Series) -> np.float64:\n",
    "    \"\"\"\n",
    "    A function to calculate the mutual information between two discrete variables,\n",
    "    assuming the two variables X and Y have multiple discrete values.\n",
    "    This approach using normalized histograms to estimate the joint and marginal distributions.\n",
    "    The sum of the joint histogram may not neccessarily be 1.\n",
    "    param X: first variable\n",
    "    param Y: second variable\n",
    "    return: mutual information between X and Y\n",
    "    \"\"\"\n",
    "    joint_prob, x_edges, y_edges = np.histogram2d(X, Y, bins=(len(set(X)), len(set(Y))), density=True)\n",
    "    prob_X = np.histogram(X, bins=len(set(X)), density=True)[0]\n",
    "    prob_Y = np.histogram(Y, bins=len(set(Y)), density=True)[0]\n",
    "\n",
    "    mi = 0\n",
    "    for i in range(len(set(X))):\n",
    "        for j in range(len(set(Y))):\n",
    "            if joint_prob[i][j] > 0:\n",
    "                mi += joint_prob[i][j] * np.log(joint_prob[i][j] / (prob_X[i] * prob_Y[j]))\n",
    "\n",
    "    return mi\n",
    "\n",
    "\n",
    "def mutual_info_with_entropy(X: pd.Series, Y: pd.Series) -> np.float64:\n",
    "    \"\"\"\n",
    "    A function to calculate the mutual information between two discrete variables.\n",
    "    This approach first calculate the entropies of each individual variable and their join entropy.\n",
    "    The mutual information is then obtained using the relation: MI(X, Y) = H(X) + H(Y) - H(X, Y)\n",
    "    The sum of the joint histogram may not neccessarily be 1.\n",
    "    param X: first variable\n",
    "    param Y: second variable\n",
    "    return: mutual information between X and Y\n",
    "    \"\"\"\n",
    "    # Calculate the individual entropies\n",
    "    h_x = entropy(np.histogram(X, bins=len(set(X)))[0] / len(X))\n",
    "    h_y = entropy(np.histogram(Y, bins=len(set(Y)))[0] / len(Y))\n",
    "\n",
    "    # Calculate the joint histogram and joint entropy\n",
    "    c_xy = np.histogram2d(X, Y, bins=(len(set(X)), len(set(Y))))[0]\n",
    "    h_xy = entropy(c_xy.reshape(-1))\n",
    "\n",
    "    # Compute the mutual information\n",
    "    mutual_info = h_x + h_y - h_xy\n",
    "    return mutual_info\n",
    "\n",
    "\n",
    "def compute_mi(data: pd.DataFrame, mi_func: Callable) -> Dict:\n",
    "    \"\"\"\n",
    "    This function compute the mutual information of each pair of variables in the dataset.\n",
    "    The results are stored in an dictionary.\n",
    "    param data: an input dataframe containing variables' values \n",
    "    param mi_func: a function to calculate the mutual information\n",
    "    return: a dictionary containing mutual information of all variable pairs.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    vars = data.columns\n",
    "    for i in range(len(vars)):\n",
    "        for j in range(i + 1, len(vars)):\n",
    "            var1 = vars[i]\n",
    "            var2 = vars[j]\n",
    "            try:\n",
    "                mi = mi_func(data[var1], data[var2])\n",
    "            except ValueError:\n",
    "                mi = mi_func(np.array(data[var1]).reshape(-1, 1), data[var2], discrete_features=True)\n",
    "            results[(var1, var2)] = mi\n",
    "    \n",
    "    # Display results\n",
    "    for var, mi_value in results.items():\n",
    "        print(f\"Mutual Information between {var[0]} and {var[1]}: {mi_value}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4/ Read and convert .h5 file to a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b  x  y\n",
       "0  0  0  1  1\n",
       "1  0  0 -1 -1\n",
       "2  1  1  1 -1\n",
       "3  1  1 -1  1\n",
       "4  0  0  1  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_h5_to_dataframe(h5_path=\"../data/raw/question2-data.h5\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5/ Calculate the Discrete Mutual Information between pairs of variables in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between a and b: 0.010833477026353552\n",
      "Mutual Information between a and x: 0.0005999350832366201\n",
      "Mutual Information between a and y: 0.00027249720152157115\n",
      "Mutual Information between b and x: 0.00017487758911443815\n",
      "Mutual Information between b and y: 2.014195200942116e-05\n",
      "Mutual Information between x and y: 0.06201744636620489\n"
     ]
    }
   ],
   "source": [
    "results0 = compute_mi(data=df, mi_func=mutual_information_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between a and b: 0.04333390810541421\n",
      "Mutual Information between a and x: 0.0011998701664732402\n",
      "Mutual Information between a and y: 0.0005449944030431423\n",
      "Mutual Information between b and x: 0.0003497551782288763\n",
      "Mutual Information between b and y: 4.028390401884232e-05\n",
      "Mutual Information between x and y: 0.06201744636620489\n"
     ]
    }
   ],
   "source": [
    "# This function is expected to be not suitable as all variables seem to be binary\n",
    "results1 = compute_mi(data=df, mi_func=mutual_information_multiple_discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between a and b: 0.010833477026353622\n",
      "Mutual Information between a and x: 0.0005999350832366357\n",
      "Mutual Information between a and y: 0.00027249720152133783\n",
      "Mutual Information between b and x: 0.0001748775891143861\n",
      "Mutual Information between b and y: 2.0141952009389286e-05\n",
      "Mutual Information between x and y: 0.062017446366204654\n"
     ]
    }
   ],
   "source": [
    "results2 = compute_mi(data=df, mi_func=mutual_info_with_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6/ Compare the results using Scikit-learn functions to calculate mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between a and b: 0.010833477026354232\n",
      "Mutual Information between a and x: 0.0005999350832371353\n",
      "Mutual Information between a and y: 0.0002724972015220595\n",
      "Mutual Information between b and x: 0.00017487758911494122\n",
      "Mutual Information between b and y: 2.0141952009777864e-05\n",
      "Mutual Information between x and y: 0.062017446366205375\n"
     ]
    }
   ],
   "source": [
    "results3 = compute_mi(data=df, mi_func=mutual_info_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information between a and b: [0.01083348]\n",
      "Mutual Information between a and x: [0.00059994]\n",
      "Mutual Information between a and y: [0.0002725]\n",
      "Mutual Information between b and x: [0.00017488]\n",
      "Mutual Information between b and y: [2.0141952e-05]\n",
      "Mutual Information between x and y: [0.06201745]\n"
     ]
    }
   ],
   "source": [
    "results4 = compute_mi(data=df, mi_func=mutual_info_classif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7/ Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach of using histogram to estimate discrete mutual information has some characteristics that may limit its efficiency and accuracy for large datasets:\n",
    "\n",
    "Nested Loops: The function uses nested loops to calculate the mutual information. This means that its time complexity is O(n^2) for the mutual information calculation, where n is the number of unique values in the datasets. For a large number of unique values, this could become a performance bottleneck.\n",
    "\n",
    "Memory Usage: The function uses histograms for both joint and marginal distributions. The size of the joint histogram is proportional to the product of the number of unique values in both variables. This can increase memory consumption substantially for datasets with many unique values.\n",
    "\n",
    "Histogram Binning: Using histograms for estimating distributions can introduce errors, especially if the bin sizes aren't chosen appropriately. In this function, the bin sizes are determined based on the number of unique values, which might not always be the best choice. For large datasets with many repeated values, this method could become inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
